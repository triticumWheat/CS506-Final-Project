{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T20:37:07.460713300Z",
     "start_time": "2024-10-19T20:36:53.923236200Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成第 1 页的爬取。\n",
      "已完成第 2 页的爬取。\n",
      "已完成第 3 页的爬取。\n",
      "已完成第 4 页的爬取。\n",
      "已完成第 5 页的爬取。\n",
      "已完成第 6 页的爬取。\n",
      "已完成第 7 页的爬取。\n",
      "已完成第 8 页的爬取。\n",
      "已完成第 9 页的爬取。\n",
      "已完成第 10 页的爬取。\n",
      "已完成第 11 页的爬取。\n",
      "已完成第 12 页的爬取。\n",
      "已完成第 13 页的爬取。\n",
      "已完成第 14 页的爬取。\n",
      "已完成第 15 页的爬取。\n",
      "已完成第 16 页的爬取。\n",
      "已完成第 17 页的爬取。\n",
      "已完成第 18 页的爬取。\n",
      "已完成第 19 页的爬取。\n",
      "已完成第 20 页的爬取。\n",
      "已完成第 21 页的爬取。\n",
      "已完成第 22 页的爬取。\n",
      "已完成第 23 页的爬取。\n",
      "已完成第 24 页的爬取。\n",
      "已完成第 25 页的爬取。\n",
      "已完成第 26 页的爬取。\n",
      "已完成第 27 页的爬取。\n",
      "已完成第 28 页的爬取。\n",
      "已完成第 29 页的爬取。\n",
      "已完成第 30 页的爬取。\n",
      "已完成第 31 页的爬取。\n",
      "已完成第 32 页的爬取。\n",
      "已完成第 33 页的爬取。\n",
      "在第 34 页找不到教室，停止爬取。\n",
      "所有页面的教室链接已经成功保存到 'classroom_links.csv' 文件中。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# 目标网址的基础部分\n",
    "base_url = \"https://www.bu.edu/classrooms/find-a-classroom/page/{}/?cts_address&cts_capacity_lfa&cts_capacity&cts_filter_submit=Search\"\n",
    "\n",
    "# 打开一个CSV文件来保存结果\n",
    "with open('../data/classroom_links.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # 写入CSV文件的标题行\n",
    "    writer.writerow(['Link'])\n",
    "\n",
    "    page = 1  # 从第一页开始\n",
    "    while True:\n",
    "        # 获取每页的URL\n",
    "        url = base_url.format(page)\n",
    "        response = requests.get(url)\n",
    "        html_content = response.content\n",
    "\n",
    "        # 使用BeautifulSoup解析网页\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # 检查是否存在<p class=\"cts-error\">No classrooms found.</p>\n",
    "        error_message = soup.find('p', class_='cts-error')\n",
    "        if error_message:\n",
    "            print(f\"在第 {page} 页找不到教室，停止爬取。\")\n",
    "            break\n",
    "\n",
    "        # 查找<ul class=\"cts-results-list\">中的<li>\n",
    "        results_list = soup.find('ul', class_='cts-results-list')\n",
    "        if results_list:\n",
    "            li_items = results_list.find_all('li')\n",
    "\n",
    "            # 提取每个<li>中的<a>标签中的href链接\n",
    "            for li in li_items:\n",
    "                link_tag = li.find('a', class_='cts-button cts-button-primary')\n",
    "                if link_tag and link_tag.get('href'):\n",
    "                    classroom_link = link_tag['href']\n",
    "                    # 写入CSV文件\n",
    "                    writer.writerow([classroom_link])\n",
    "\n",
    "        # 输出当前爬取的页码信息\n",
    "        print(f\"已完成第 {page} 页的爬取。\")\n",
    "        page += 1  # 移动到下一页\n",
    "\n",
    "print(\"所有页面的教室链接已经成功保存到 'classroom_links.csv' 文件中。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T05:42:07.386455Z",
     "start_time": "2024-11-01T05:35:32.311559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "教室信息已成功保存到 'classroom_data.json' 文件中。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# 从CSV中读取所有链接\n",
    "urls = []\n",
    "with open('../data/classroom_links.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for row in reader:\n",
    "        urls.append(row[0])\n",
    "\n",
    "# 用于存储所有页面信息的列表\n",
    "classroom_data = []\n",
    "\n",
    "# 遍历每个URL并获取页面信息\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "\n",
    "    # 使用BeautifulSoup解析网页\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # 1. 提取Name属性，来自<div class=\"content-panel\">中的<h1>元素\n",
    "    name = soup.find('div', class_='content-panel').find('h1').get_text().strip()\n",
    "\n",
    "    # 2. 查找<div class=\"cts-detail-container\">并提取其中的信息\n",
    "    detail_container = soup.find('div', class_='cts-detail-container')\n",
    "    details = detail_container.find_all('div', class_='cts-detail-details')\n",
    "\n",
    "    classroom_info = {\n",
    "        \"Name\": name,\n",
    "        \"Details\": {},\n",
    "        \"AdditionalInfo\": {}\n",
    "    }\n",
    "\n",
    "    # 处理第一个 cts-detail-details，提取每个<h4>和对应的<ul>\n",
    "    first_detail = details[0] if len(details) > 0 else None\n",
    "    if first_detail:\n",
    "        h4_elements = first_detail.find_all('h4')\n",
    "\n",
    "        for h4 in h4_elements:\n",
    "            h4_title = h4.get_text().strip()\n",
    "\n",
    "            # 找到<h4>后面的第一个<ul>\n",
    "            ul_list = h4.find_next('ul', class_='cts-detail-list')\n",
    "            if ul_list:\n",
    "                li_items = ul_list.find_all('li')\n",
    "                li_texts = []\n",
    "                for li in li_items:\n",
    "                    li_text = li.get_text(strip=True).replace(li.find('span').get_text(strip=True), \"\").strip()\n",
    "                    li_texts.append(li_text)\n",
    "\n",
    "                classroom_info[\"Details\"][h4_title] = li_texts\n",
    "\n",
    "    # 处理第二个 cts-detail-details，提取6个<li>中的meta-name和meta-value\n",
    "    second_detail = details[1] if len(details) > 1 else None\n",
    "    if second_detail:\n",
    "        li_items = second_detail.find_all('li')\n",
    "        for li in li_items:\n",
    "            meta_name = li.find('span', class_='meta-name').get_text(strip=True)\n",
    "            meta_value = li.find('span', class_='meta-value').get_text(strip=True)\n",
    "            classroom_info[\"AdditionalInfo\"][meta_name] = meta_value\n",
    "\n",
    "    # 将该教室的信息加入列表\n",
    "    classroom_data.append(classroom_info)\n",
    "\n",
    "# 将数据保存为JSON文件\n",
    "with open('../data/classroom_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(classroom_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"教室信息已成功保存到 'classroom_data.json' 文件中。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
